Chapter 7:  Leaf-Spine Fabric
=======================================

This chapter describes Trellis, a leaf-spine switching fabric
implemented by a collection of control applications running on top of
ONOS. We have introduced various aspects of Trellis in earlier
chapters, but we summarize the highlights here before getting into the
details.

* Trellis supports the leaf-spine fabric topology that is commonly
  used to interconnect multiple racks of servers in a datacenter (see
  :numref:`Figure %s <fig-leaf-spine>`), but it also supports
  multi-site deployments (see :numref:`Figure %s <fig-trellis>`).
  Trellis uses only white-box switches to build out the fabric. It can
  run on a mix of fixed-function and programmable pipelines, but is
  running in production with the former.

* Trellis supports a wide-range of L2/L3 features, all re-implemented
  as SDN control apps (with the exception of Quagga, which is used to
  exchange BGP routes with external peers). Trellis implements L2
  connectivity within each server rack, and L3 connectivity between
  racks.

* Trellis supports access/edge networking technologies, such as PON
  and RAN, including support for (a) routing IP traffic to/from devices
  connected to those access networks, and (b) off-loading access
  network functionality into the fabric switches.

This chapter does not give a comprehensive description of every
Trellis feature, but it does illustrate some of the thought processes
involved in building a comprehensive and production-grade network
using SDN.

7.1 Feature Set
---------------

SDN provides an opportunity to customize the network, but for
pragmatic reasons, the first requirement for adoption is to reproduce
functionality that already exists, and do so in a way that reproduces
the resilience and scalability of legacy solutions. Trellis has
satisfied this requirement, which we summarize here.

First, with respect to L2 connectivity, Trellis supports VLANs,
including native support for forwarding traffic based on just an outer
VLAN id, as well as QinQ support based on an outer/inner VLAN id
pair. Support for QinQ is particularly relevant to access networks,
where double tagging is used to isolate traffic belonging to different
service classes. In addition, Trellis supports L2 tunnels across the
L3 fabric (both single and double tagged).

Second, with respect to L3 connectivity, Trellis supports IPv4 and
IPv6 routing for both unicast and multicast addresses. For the latter,
Trellis implements centralized multicast tree construction (as opposed
to running a protocol like PIM), but does include IGMP support for end
hosts wishing to join/leave multicast groups. Trellis also supports
both ARP (for IPv4 address translation) and NDP (for IPv6 neighbor
discovery), along with support for both DHCPv4 and DHCPv6.

Third, Trellis provides high availability in the face of link or
switch failures. It does this through a combination of well-known
techniques: dual-homing, link binding, and ECMP link groups. As
illustrated in :numref:`Figure %s <fig-netconfig>`, each server in a
Trellis cluster is connected to a pair of ToR (leaf) switches, where
the OS running on each compute server implements active-active link
bonding. Each leaf switch is then connected by a pair of links to a
pair of spine switches, with an ECMP group defined for the pair of
links connecting each leaf to a given spine and for the set of links
connecting each leaf to a pair of spines. The cluster as a whole then
has multiple connections to external routes, shown via leaf switches 3
and 4 in the Figure. Not shown in :numref:`Figure %s <fig-netconfig>`
is the fact that Trellis runs on top of ONOS, which is itself
replicated for the sake of availability. In a configuration like the
one shown here, ONOS (and hence the Trellis control applications) are
replicated on three to five servers.

.. _fig-netconfig:
.. figure:: figures/Slide31.png
    :width: 400px
    :align: center

    High availability through a combination of dual-homing, link
    bonding, and ECMP groups.

Details about link aggregation and ECMP are beyond the scope of this
book, but the idea is straightforward: the packet forwarding mechanism
is augmented to load balance outgoing packets among a group (e.g., a
pair) of links (egress ports) rather than having just a single “best”
output link (egress port). This both improves bandwidth and results in
an automatic recover mechanism should any single link fail.

Fourth, with respect to scalability, Trellis has demonstrated the
ability to support up to 120k routes and 250k flows. This is in a
configuration that includes two spine switches and eight leaf
switches, the latter implying up to four racks of servers. As with
availability, Trellis’s ability to scale performance is directly due
to ONOS’s ability to scale.

7.2 Segment Routing
-------------------

The previous section focused on *what* Trellis does. This section
focuses on *how*, where the core strategy is based on *Segment Routing
(SR)*. The term “segment routing” comes from the idea that the
end-to-end path between any pair of hosts can be defined by a sequence
of segments, where label-switching is used to traverse a sequence of
segments along an end-to-end path. The idea is an application of
*Multi-Protocol Label Switching (MPLS)*, which you can read more about
here:

.. _reading_mpls:
.. admonition:: Further Reading

   `Multi-Protocol Label Switching
   <https://book.systemsapproach.org/scaling/mpls.html>`__. *Computer
   Networks: A Systems Approach*, 2020.

When applied to a leaf-spine fabric, there are always two segments
involved—leaf-to-spine and spine-to-leaf—where Tellis programs the
switches to match and then push/pop MPLS labels.  :numref:`Figure %s
<fig-sr>` illustrates how SR works in Trellis using a simple
configuration that forwards traffic between a pair of hosts: 10.0.1.1
and 10.0.2.1. In this example, the servers connected to Leaf 1 are on
subnet 10.0.1/24, the servers connected to Leaf 2 are on subnet
10.0.2/24, and each of the switches have an assigned MPLS id: 101,
103, 102, and 104.

.. _fig-sr:
.. figure:: figures/Slide32.png
    :width: 400px
    :align: center

    Example of Segment Routing being used to forward traffic between a
    pair of hosts.

When Host 1 sends a packet with destination address 10.0.2.1 it is by
default forwarded to the server’s ToR/leaf switch. (Because of link
aggregation the packet could show up at either ToR, but the behavior
is exactly the same for both.) Leaf 1 matches the destination IP
address, learns this packet needs to cross the fabric and emerge at
Leaf 2 to reach subnet 10.0.2/24, and so pushes the MPLS label 102
onto the packet. Because of ECMP, Leaf 1 can forward the resulting
packet to either spine, at which point that switch matches the MPLS
label 102, pops the label off the header, and forwards it to Leaf 2.
Finally, Leaf 2 matches the destination IP address and forwards the
packet along to Host 2.

What you should take away from this example is that SR is highly
stylized. For a given combination of leaf and spine switches, Trellis
pre-computes the possible paths and installs the corresponding
match/action rules in the underlying switches. The complexity having
to do with balancing load across multiple paths is delegated to ECMP,
which is similarly unaware of any end-to-end paths. From an
implementation perspective, the Trellis control application that
implements SR passes these match/action rules to ONOS, which in turn
installs them on the underlying switches. Trellis also maintains its
own Atomix map to manage the set of ECMP groups connecting leaf and
spine switches.

Finally, Trellis takes advantage of the Host, Route, and Mcast
services implemented by ONOS. These are used to determine which IP
subnet each host is connected to, which switches serve each IP prefix,
and where to find all the hosts connected to each multicast group,
respectively. These are all relatively straightforward because Trellis
imposes the simplifying constraint that each rack corresponds to
exactly one IP subnet. Trellis does not run distributed protocols like
BGP or PIM to learn about routes or multicast trees, but instead
computes the right answers based on global information, and directly
pushes the mappings to the Route an Mcast services, respectively. For
any routes that are external to Trellis (i.e., the route is learned
through BGP), the corresponding prefix to nexthop mapping is still
passed to ONOS through the Route service, indicating that the
destination prefix is reachable via the leaf switches that connect the
fabric to upstream routers (e.g., Switches 3 and 4 in :numref:`Figure
%s <fig-netconfig>`).
